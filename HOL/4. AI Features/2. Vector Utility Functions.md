# Vector Utility Functions

In modern applications—especially those involving AI or machine learning—vector operations are fundamental. SQL Server 2025 introduces built-in vector utility functions to help you analyze and prepare vector data directly in the database.

Embeddings (vectors) are available from a variety of sources, including:
* Azure OpenAI models (e.g., `text-embedding-large`), as we are using in all the labs in this workshop
* Open-source models like BERT, SentenceTransformers, and others
* Custom models trained on your own data
* Pre-computed embeddings from third-party services
* Any other source that produces vector representations of data

Depending on the source of your vectors, you may need to perform operations like normalization, magnitude calculation, and metadata inspection before using them in similarity searches or DiskANN indexing (which you'll learn about in the next lab).

In this lab, you will:

* Compute the magnitude (norm) of vectors with `VECTOR_NORM()`
* Convert raw ("unnormalized") vectors into "unit" vectors using `VECTOR_NORMALIZE()`
* Inspect vector metadata (dimension, length, data type) via `VECTORPROPERTY()`

These functions are essential for tasks such as similarity scoring, data normalization prior to DiskANN indexing, and validating vector integrity.

By the end of this lab, you will be able to:

1. Calculate vector magnitudes for use in similarity calculations.
2. Normalize vectors to unit length, preparing them for simlarity calculations and DiskANN indexes.
3. Retrieve metadata about vector columns to validate data before indexing or analysis.

> **Note:** Azure OpenAI embeddings (such as the vectors returned by the `text-embedding-large` model in our previous lab) are already normalized to unit length. So you did not need to use any of these functions on those vectors in the earlier lab. However, many other models (like BERT or SentenceTransformers) return unnormalized vectors, which you will need to normalize before using them in similarity calculations or DiskANN indexing.

## Create the DemoVectors Table

We need a table to hold sample vector data. This simulates the kind of vector embeddings you might store after generating them from text, images, or other media.

In SQL Server Management Studio (SSMS), right-click on the **HolDb** database in Object Explorer and select **New Query** to open a new query window for this lab.

Now run the following script to create and populate a `DemoVectors` table with 4-dimensional vectors:

```sql
CREATE TABLE VectorDemo (
  VectorDemoId int PRIMARY KEY,
  RawVector vector(4)
)

INSERT INTO VectorDemo VALUES
  (1, '[1.0, 2.0, 2.0, 1.0]'),
  (2, '[0.0, 3.0, 4.0, 0.0]'),
  (3, '[2.0, 2.0, 2.0, 2.0]')
```

Notice how we define the `RawVector` column as `vector(4)`, indicating that each vector has 4 dimensions. The vectors are stored in a compact binary format internally, and represented as JSON arrays for convenience.

## Compute Norms

The **norm** (or magnitude) of a vector is a key component in cosine similarity and distance calculations.

The `VECTOR_NORM` function in SQL Server computes the *magnitude* of a vector using one of three different methods: `norm1`, `norm2`, and `norminf`. Each method measures the size of the vector in a different mathematical sense, corresponding to the L¹, L², and L∞ norms respectively. Thus, each provides different insight into the vector's shape or influence.

### Vector Norm Types

| Norm Type | Description | Formula | Use Case |
| - | - | - | - |
| `norm1` | **Manhattan norm** (L¹ norm, aka "taxicab geometry") – sum of absolute values of all elements. | x₁ + x₂ + ... + xₙ | Measure the **total "weight"** of the vector (often used in sparse data, or when dealing with taxicab geometry) |
| `norm2` | **Euclidean norm** (L² norm, most common for RAG applications) – the straight-line distance from the origin to the point in n-dimensional space. | √(x₁² + x₂² + ... + xₙ²) | Measure the **overall magnitude** of a vector (e.g., for similarity or distance calculations |
| `norminf` | **Maximum norm** (Infinity norm, L∞) – the largest absolute value among all elements. | max(x₁, x₂, ... xₙ) | the **maximum deviation** along any single dimension |

Run this code to compute the magnitude of each vector in the table, using `VECTOR_NORM` with each of the three available vector norm types:

```sql
SELECT
  VectorDemoId,
  RawVector,
  Norm1   = VECTOR_NORM(RawVector, 'norm1'),
  Norm2   = VECTOR_NORM(RawVector, 'norm2'),
  NormInf = VECTOR_NORM(RawVector, 'norminf')
FROM
  VectorDemo
```

Observe the results:

| Vector | `norm1` | `norm2` | `norminf` |
| ------ | ------- | ------- | --------- | 
| **`[1.0, 2.0, 2.0, 1.0]`** | 1 + 2 + 2 + 1 =<br>**6** | √(1² + 2² + 2² + 1²) =<br>√(1 + 4 + 4 + 1) =<br>√10 ≈<br>**3.1623** | max(1, 2, 2, 1) =<br>**2** |
| **`[0.0, 3.0, 4.0, 0.0]`** | 0 + 3 + 4 + 0 =<br>**7** | √(0² + 3² + 4² + 0²) =<br>√(0 + 9 + 16 + 0) =<br>√25 =<br>**5** | max(0, 3, 4, 0) =<br>**4** |
| **`[2.0, 2.0, 2.0, 2.0]`** | 2 + 2 + 2 + 2 =<br>**8** | √(2² × 2² × 2² × 2²) =<br>√(4 × 4 × 4 × 4) =<br>√16 =<br>**4** | max(2, 2, 2, 2) =<br>**2** |

Notice how none of these vectors have a magnitude of 1. This is important because many scenarios (like cosine similarity, and indexing with DiskANN) require vectors to be normalized to *unit length* (magnitude of 1) to provide correct results.

## Normalize Vectors

**Normalization** rescales a vector so that its magnitude is 1, without changing its direction. This is especially useful for tasks like *cosine similarity*, where the direction of a vector matters more than its magnitude. 

### Direction vs. Magnitude

When comparing vectors, their *direction* is often more important than their *magnitude*. For example, two vectors pointing in the same direction but with different lengths (magnitudes) are considered similar, while two vectors of equal length (magnitude) but pointing in opposite directions are not.

Direction versus magnitude refers to the distinction between *where* a vector points (its direction) and *how long* it is (its magnitude or length). Two vectors with different magnitudes can still point in the same direction and thus represent the same underlying pattern or meaning — just at a different scale. Normalization removes magnitude (that is, it "rescales" a vector) so that comparisons focus only on direction.

For example, the vectors `[1, 2, 2, 1]` and `[2, 4, 4, 2]` appear different at first glance, but they point in the same direction — the second is just a scaled-up version of the first. After normalization, both reduce to `[0.2, 0.4, 0.4, 0.2]`, revealing that they represent the same pattern. In contrast, `[1, 2, 2, 1]` and `[0, 3, 4, 0]` have similar magnitudes but point in very different directions. Even after normalization, they remain dissimilar, highlighting that their underlying patterns are fundamentally different.

The `VECTOR_NORMALIZE` function in SQL Server returns a new vector with unit length (that is, with a magnitude of 1), computed using one of the three norms you just learned about with the `VECTOR_NORM` function: `norm1`, `norm2`, or `norminf`. This corresponds to dividing each component of the vector by its L¹, L², or L∞ norm, respectively.

### Normalization Methods

| Norm Type | Description                                                       | Formula                       |
| --------- | ----------------------------------------------------------------- | ----------------------------- |
| `norm1`   | Divide each element by the sum of absolute values. | xᵢ / (x₁ + x₂ + ... + xₙ)     |
| `norm2`   | Divide each element by the Euclidean norm.         | xᵢ / √(x₁² + x₂² + ... + xₙ²) |
| `norminf` | Divide each element by the largest absolute value. | xᵢ / max(x₁, x₂, ..., xₙ)     |

Run this code to normalize each vector using `VECTOR_NORMALIZE` and all three normalization types:

```sql
SELECT
  VectorDemoId,
  RawVector,
  NormalizedNorm1   = VECTOR_NORMALIZE(RawVector, 'norm1'),
  NormalizedNorm2   = VECTOR_NORMALIZE(RawVector, 'norm2'),
  NormalizedNormInf = VECTOR_NORMALIZE(RawVector, 'norminf')
FROM
  VectorDemo
```

Observe how the normalized vectors are now unit vectors (magnitude of 1) for each normalization type:

| Vector                     | `norm1` Normalized                                               | `norm2` Normalized                                                       | `norminf` Normalized                                  |
| -------------------------- | ---------------------------------------------------------------- | ------------------------------------------------------------------------ | ----------------------------------------------------- |
| **`[1.0, 2.0, 2.0, 1.0]`** | \[1/6, 2/6, 2/6, 1/6] =<br>**\[0.1667, 0.3333, 0.3333, 0.1667]** | \[1/√10, 2/√10, 2/√10, 1/√10] =<br>**\[0.3162, 0.6325, 0.6325, 0.3162]** | \[1/2, 2/2, 2/2, 1/2] =<br>**\[0.5, 1.0, 1.0, 0.5]**  |
| **`[0.0, 3.0, 4.0, 0.0]`** | \[0/7, 3/7, 4/7, 0/7] =<br>**\[0.0, 0.4286, 0.5714, 0.0]**       | \[0/5, 3/5, 4/5, 0/5] =<br>**\[0.0, 0.6, 0.8, 0.0]**                     | \[0/4, 3/4, 4/4, 0/4] =<br>**\[0.0, 0.75, 1.0, 0.0]** |
| **`[2.0, 2.0, 2.0, 2.0]`** | \[2/8, 2/8, 2/8, 2/8] =<br>**\[0.25, 0.25, 0.25, 0.25]**         | \[2/4, 2/4, 2/4, 2/4] =<br>**\[0.5, 0.5, 0.5, 0.5]**                     | \[2/2, 2/2, 2/2, 2/2] =<br>**\[1.0, 1.0, 1.0, 1.0]**  |

Let's confirm that the normalization worked, and has indeed transformed each vector to a *unit* vector. To do this, you can use `VECTOR_NORM` to compute the magnitude of the normalized vectors, and verify that each vector is now normalized with a magnitude of 1:

```sql
SELECT
  VectorDemoId,
  RawVector,
  NormalizedNorm1   = VECTOR_NORM(VECTOR_NORMALIZE(RawVector, 'norm1'), 'norm1'),
  NormalizedNorm2   = VECTOR_NORM(VECTOR_NORMALIZE(RawVector, 'norm2'), 'norm2'),
  NormalizedNormInf = VECTOR_NORM(VECTOR_NORMALIZE(RawVector, 'norminf'), 'norminf')
FROM
  VectorDemo
```

Observe the results:

| RawVector             | NormalizedNorm1 | NormalizedNorm2 | NormalizedNormInf |
| --------------------- | ---------------- | ---------------- | ----------------- |
| **`[1.0, 2.0, 2.0, 1.0]`** | 1.00000002980232 | 0.999999993278206 | 1               |
| **`[0.0, 3.0, 4.0, 0.0]`** | 1.00000002980232 | 1.00000002384186 | 1               |
| **`[2.0, 2.0, 2.0, 2.0]`** | 1              | 1              | 1               |
        
Notice now that each vector's magnitude is now 1 (or almost exactly 1), confirming that the normalization worked correctly. The tiny deviations from exactly 1 (e.g., 1.00000002980232, 0.999999993278206) are due to floating-point precision errors in calculating the magnitude, not an issue with the `VECTOR_NORMALIZE` or `VECTOR_NORM` functions themselves.

* **When and Why to Normalize**

  Normalization is typically used when you want to **compare vectors based on direction, not magnitude**. This is essential for techniques like **cosine similarity**, where similarity is measured by the angle between two vectors, not their raw size.

* **Azure OpenAI Embeddings (e.g., `text-embedding-large`)**

  These models return **L²-normalized vectors by default**. Each vector already has a unit length, so you do **not** need to call `VECTOR_NORMALIZE` again before performing similarity searches using cosine distance.

* **Other Models (e.g., BERT, SentenceTransformers)**

  Many open-source models return **unnormalized vectors**. In these cases, you **must** normalize them (usually with `'norm2'`) if you're planning to:

  * Use cosine similarity (e.g., for RAG applications)
  
  * Index the vectors in a structure like DiskANN, which expects normalized input

Failing to normalize these vectors may lead to **incorrect similarity results**, because larger-magnitude vectors could dominate the calculation, even if they point in a similar direction.

### Summary

* Use `norm1` normalization when comparing proportional contribution across components (e.g., in sparse vectors or distributions).
* Use `norm2` normalization for cosine similarity or when working with DiskANN indexes, which require unit-length vectors. This is the most common and recommended method for RAG applications.
* Use `norminf` normalization to emphasize relative scale compared to the largest component, useful in bounded comparison scenarios.

If you're unsure whether your model outputs normalized vectors, check the documentation or compute `VECTOR_NORM(vector, 'norm2')` — if the result is 1 for all vectors, they're already normalized. If not, then normalize them before comparing.

## Inspect Vector Properties

Before performing operations like similarity search or distance calculations, it's often important to verify that two vectors are **compatible** — that is, they have the same number of dimensions and the same base data type. SQL Server provides the `VECTORPROPERTY` function to let you inspect this metadata at runtime.

This can be useful for:

* **Validating input vectors** before running vector functions like `VECTOR_DISTANCE` or `VECTOR_NORM`
* **Debugging** mismatches in dimension (e.g., comparing 1536-dimensional embeddings with 768-dimensional ones)
* **Confirming data types** when loading or transforming vectors dynamically

### Available Vector Properties

| Property       | Description                                                          |
| -------------- | -------------------------------------------------------------------- |
| `'Dimensions'` | Returns the number of elements (i.e., dimensionality) of the vector  |
| `'BaseType'`   | Returns the underlying data type of each component (e.g., `float32`) |

Run this query to inspect the properties of each vector in the demo table:

```sql
SELECT
  VectorDemoId,
  RawVector,
  Dimensions = VECTORPROPERTY(RawVector, 'Dimensions'),
  BaseType   = VECTORPROPERTY(RawVector, 'BaseType')
FROM
  VectorDemo
```

Observe the results:

| VectorDemoId | RawVector             | Dimensions | BaseType |
| ------------ | --------------------- | ---------- | -------- |
| 1            | \[1.0, 2.0, 2.0, 1.0] | 4          | float32  |
| 2            | \[0.0, 3.0, 4.0, 0.0] | 4          | float32  |
| 3            | \[2.0, 2.0, 2.0, 2.0] | 4          | float32  |

This output shows that all vectors in the `RawVector` column have:
* **4 dimensions** (i.e., they are 4-dimensional vectors)
* **Base type of `float32`** (i.e., each component is a 32-bit floating-point number)
* This is the expected configuration for these vectors, which were created with the `vector(4)` type.

### Why This Matters

When working with vector functions, **dimension mismatches will cause runtime errors**. For example, you cannot compute the distance between a 768-dimensional vector and a 1536-dimensional one. Likewise, attempting to insert a `VECTOR(5)` into a column declared as `VECTOR(4)` will fail.

Ensuring that two vectors have the same number of dimensions and use the same base type (`float32` or `float64`) is essential for correctness and performance in vector search, RAG, and similarity-based applications.

## Cleanup

To clean up, delete the demo table:

```sql
DROP TABLE VectorDemo
```
___

▶ [Lab: DiskANN Indexing](https://github.com/lennilobel/sql2025-workshop-hol-orlando2025/blob/main/HOL/4.%20AI%20Features/3.%20DiskANN%20Indexing.md)
